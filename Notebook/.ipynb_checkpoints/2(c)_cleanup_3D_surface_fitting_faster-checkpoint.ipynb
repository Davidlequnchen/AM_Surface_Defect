{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point cloud based surface defect detection and localisation\n",
    "\n",
    "### Author: Chen Lequn\n",
    "#### Initial commit: 13 Jan, 2021\n",
    "\n",
    "## Project description and objectives:\n",
    "- Preprocessing the point cloud data: \n",
    "1. data standarisation - centered at zero coordinate; \n",
    "2. Data labeling: add a fourth dimension to the dataset - marking all the points to \"non-defective (flat)\"; \"machining (bulge)\"; \"AM repair (dent)\"\n",
    "- 3D shape fitting feature extraction:\n",
    "1. calculating surface normal vector and principle curvatures.\n",
    "2. Methdologies (candidate): PCA, Jet approximation, DeepFit Network (based on PointNet, ECCV 2020)\n",
    "- Segmentation/Classification:\n",
    "1. Automatic point cloud segmentation based on PointNet\n",
    "2. Explore various point cloud classification/segmentation algorithms from recent years 2017-2021 CVPR/IROS \n",
    "\n",
    "## **Notebook 2 (c): cleaned up and faster computation version: point cloud shape fitting and feature extraction analysis**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook aims to estimate normal vectors and principal curvatures from 3D point clouds using classic PCA and Jet fitting methods as well as recent DeepFit method. \n",
    "\n",
    "\n",
    "Surface normals and curvatures are a very important properties in shape analysis and are widely used in different fields like computer graphics, computer vision and more. \n",
    "\n",
    "Such features are useful for noise reduction, point cloud segmentation, and classification.\n",
    "\n",
    "\n",
    "### Reference: \n",
    "- **DeepFit(ECCV 2020)** Network: 3D surface fitting https://github.com/sitzikbs/DeepFit\n",
    "- **Nesti-Net**: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks: https://github.com/sitzikbs/Nesti-Net#nesti-net-normal-estimation-for-unstructured-3d-point-clouds-using-convolutional-neural-networks\n",
    "* [PCL normal estimation using PCA](http://pointclouds.org/documentation/tutorials/normal_estimation.php)\n",
    "* [CGAL normal estimation using Jets](https://doc.cgal.org/latest/Jet_fitting_3/index.html#Jet_fitting_3Mathematical)\n",
    "* [DeepFit paper](https://arxiv.org/pdf/2003.10826.pdf)\n",
    "* [Jet fitting paper](https://graphics.stanford.edu/courses/cs468-03-fall/Papers/cazals_jets.pdf)\n",
    "* PCPNet and Dataset: http://geometry.cs.ucl.ac.uk/group_website/projects/2018/pcpnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mathematical Background\n",
    "3D point clouds are represented as a matrix of $(x, y, z)$ coordinates of points in 3D space. \n",
    "If you are unfamiliar with plane fitting and jets, this section provides a short background. For more in-depth information see the refrences at the bottom. \n",
    "\n",
    "The following methods share the following: \n",
    "* Input: 3D point cloud  + querry point $q_i$\n",
    "* Find $q_i$'s k nearest neighbors.  \n",
    "* Do something fancey :)\n",
    "* Output: Normal vector at  query point $N_{q_i}$\n",
    "\n",
    "For estimating the normal at each point we use each point in the point cloud as the query point. \n",
    "\n",
    "\n",
    "#### PCA\n",
    "In this method we estimate the tangent plane to the underlying surface at the query point. This boils down to  solving the eigenvalue and eigenvector decomposition of the covariance matrix created from the points nearest neighbors:\n",
    "\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{k}\\sum_{i=1}^{k}(p_i-\\hat{p})(p_i-\\hat{p})^T\n",
    "\\end{equation}\n",
    "\n",
    "Here $p_i$ are the neighboring points and $\\hat{p}$ is the neighbours centroid. \n",
    "The normal vector is the eugenvector associated with the smallest eigenvalue. \n",
    "\n",
    "Using this method we cannot directly estimate the principal curvatures (the principal curvatures of a plane are 0).\n",
    "\n",
    "\n",
    "#### Jet fitting\n",
    "\n",
    " An $n$-jet of the height function over a surface is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x,y)=J_{\\beta,n}(x,y)= \\sum_{k=0}^{n}\\sum_{j=0}^{k}\\beta_{k-j,j}x^{k-j}y^j\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\beta$ is the jet coefficients vector that consists of $N_n=(n+1)(n+2)/2$ terms.\n",
    "\n",
    "We require that every point satisfy the equation above, yielding the system of linear equations:\n",
    "\\begin{equation}\n",
    "    M\\beta = B\n",
    "\\end{equation}\n",
    "\n",
    "It is well known that the solution can be expressed in closed-form as: \n",
    "\\begin{equation}\n",
    "    \\beta = (M^TM)^{-1}M^TB\n",
    "\\end{equation}\n",
    "\n",
    "Here $M=(1, x_i, y_i, ..., x_i y_i^{n-1}, y_i^n)_{i=1,...,N_p}\\in \\mathbb{R}^{N_p \\times N_n}$ is the Vandermonde matrix and the height function vector $B=(z_1, z_2,...z_{N_p})^T \\in \\mathbb{R}^N_p$. Both represent the sampled points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library for DeepFit utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Imports\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda (GPU) for training\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../DeepFit/utils')\n",
    "sys.path.insert(0, '../DeepFit/models')\n",
    "sys.path.insert(0, '../DeepFit/trained_models')\n",
    "\n",
    "import DeepFit\n",
    "import tutorial_utils as tu\n",
    "import torch\n",
    "import ipyvolume as ipv\n",
    "import pylab\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import functools\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "# %matplotlib auto\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# gpu_idx = 0\n",
    "# device = torch.device(\"cpu\" if gpu_idx < 0 else \"cuda:%d\" % 0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    print (\"Using cuda (GPU) for training\")\n",
    "else:\n",
    "    is_cuda = False\n",
    "    print (\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"..\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"png_images\")\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"dataset\")\n",
    "XYZ_point_PATH = os.path.join(DATA_PATH, \"xyz\")\n",
    "file_output_dir =  os.path.join(PROJECT_ROOT_DIR, \"output_dir/surface_fitting_result\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "os.makedirs(file_output_dir, exist_ok=True)\n",
    "\n",
    "## function for automatically save the diagram/graph into the folder \n",
    "def save_fig_plt(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "def save_fig_pylab (fig_name, tight_layout=True, fig_extension=\"png\"):\n",
    "    path = os.path.join(IMAGES_PATH, fig_name + \".\" + fig_extension)\n",
    "    print(\"Saving figure \", fig_name)\n",
    "    # if tight_layout:\n",
    "    #     plt.tight_layout()\n",
    "    ipv.pylab.savefig(path)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 16 20:32:50 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P0    24W /  N/A |    121MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading point cloud data of a AM part surface\n",
    "\n",
    "- **[Data Standarization]**: (SinglePointCloudDataset function) shrink the data to unit sphere (centered at zero, unit variance)\n",
    "- **It is important to conduct data standarization for faster computation, however, we disabled it temporarily for now. To enable this, uncomment the line in SinglePointCloudDataset function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39026585 -0.46096367 -0.10555083]\n",
      " [ 0.39141718 -0.43291363 -0.10771554]\n",
      " [ 0.39170888 -0.4258814  -0.10768397]\n",
      " ...\n",
      " [ 0.2594107   0.7971974   0.07942393]\n",
      " [ 0.2695284   0.7980416   0.07940814]\n",
      " [ 0.29066217  0.79756594  0.07974651]]\n"
     ]
    }
   ],
   "source": [
    "point_cloud_dataset = tu.SinglePointCloudDataset(os.path.join(XYZ_point_PATH, \"S_seg.xyz\"), points_per_patch=256)\n",
    "print(point_cloud_dataset.points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit an n-jet (and compute normal vector)\n",
    "This may take a while, depending on the number of points. \n",
    "\n",
    "Note: this is the classic jet fitting method which uses non-weighted least squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Polynomial order unsupported, please use 1 or 2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13628/473543454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mscale_radius\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale_radius\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     beta, n_est, neighbors_n_est = DeepFit.fit_Wjet(points, torch.ones_like(points[:, 0]), order=jet_order,\n\u001b[0m\u001b[0;32m     13\u001b[0m                                compute_neighbor_normals=False)\n\u001b[0;32m     14\u001b[0m     \u001b[0mn_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cancel out pca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github_repository\\AM_thermal_acoustic_defect_detection\\AM_Surface_Defect\\Notebook\\../DeepFit/models\\DeepFit.py\u001b[0m in \u001b[0;36mfit_Wjet\u001b[1;34m(points, weights, order, compute_neighbor_normals)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mD_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Polynomial order unsupported, please use 1 or 2 \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mXtX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mw_vector\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Polynomial order unsupported, please use 1 or 2 "
     ]
    }
   ],
   "source": [
    "jet_order =4\n",
    "dataloader = torch.utils.data.DataLoader(point_cloud_dataset, batch_size=256, num_workers=8)\n",
    "\n",
    "for batchind, data in enumerate(dataloader, 0):\n",
    "    points = data[0]\n",
    "    data_trans = data[1]\n",
    "    scale_radius = data[-1]\n",
    "    points = points.to(device)\n",
    "    data_trans = data_trans.to(device)\n",
    "    scale_radius = scale_radius.to(device)\n",
    "    \n",
    "    beta, n_est, neighbors_n_est = DeepFit.fit_Wjet(points, torch.ones_like(points[:, 0]), order=jet_order,\n",
    "                               compute_neighbor_normals=False)\n",
    "    n_est = torch.bmm(n_est.unsqueeze(1), data_trans.transpose(2, 1)).squeeze(dim=1) # cancel out pca\n",
    "    n_est = n_est.detach().cpu()\n",
    "    normals = n_est if batchind==0 else torch.cat([normals, n_est], 0)\n",
    "    \n",
    "    curv_est, principal_dirs = tu.compute_principal_curvatures(beta)\n",
    "    curv_est = curv_est / scale_radius.unsqueeze(-1).repeat(1, curv_est.shape[1])\n",
    "    curv_est = curv_est.detach().cpu()\n",
    "    curvatures = curv_est if batchind == 0 else torch.cat([curvatures, curv_est], 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normals are unoriented so we will now flip them outwards (assuming that the origin is an internal point and the shape is simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_sign = torch.sign(torch.sum(normals*torch.tensor(point_cloud_dataset.points), dim=1)).unsqueeze(-1)\n",
    "normals = n_sign * normals\n",
    "normals = normals.detach().cpu()\n",
    "curvatures = n_sign.repeat([1, 2]) * curvatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the point cloud (n-Jet fitting Results)\n",
    "We plot the point cloud and allow for 3 color overlays:\n",
    "* Solid - (all points have the same color)\n",
    "* Normals - We map the normal vectors to the RGB cube and use these values for coloring the point cloud.\n",
    "* Curvatures - We map the principal curvatures to the XXX colormap to RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Visualize normals\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_n = tu.normal2rgb(normals.numpy()) #convert normal vectors to RGB\n",
    "curvature_range_min = [-torch.mean(torch.abs(curvatures[:, 1])) - torch.std(torch.abs(curvatures[:, 1])), \n",
    "                                torch.mean(torch.abs(curvatures[:, 1])) + torch.std(torch.abs(curvatures[:, 1]))]\n",
    "curvature_range_max = [-torch.mean(torch.abs(curvatures[:, 0])) - torch.std(torch.abs(curvatures[:, 0])), \n",
    "                                torch.mean(np.abs(curvatures[:, 0])) + torch.std(torch.abs(curvatures[:, 0]))]                                   \n",
    "color_curv = tu.curvatures2rgb(curvatures.clone(),  k1_range=curvature_range_max, k2_range=curvature_range_min)\n",
    "\n",
    "# make some buttons to toggle between the colors \n",
    "btn_pc = widgets.Button(description='Solid')\n",
    "btn_pc.style.button_color='lightgray'\n",
    "btn_n = widgets.Button(description='normals')\n",
    "btn_n.style.button_color='lightgray'\n",
    "btn_c = widgets.Button(description='Curvatures')\n",
    "btn_c.style.button_color='lightgray'\n",
    "\n",
    "def update_pc_color_to_solid(b, scatter_h):\n",
    "    \"\"\"    \n",
    "        this function is linked to the buttons and updates the the point cloud color\n",
    "    \"\"\"\n",
    "    scatter_h.color = 'red'\n",
    "\n",
    "    \n",
    "def update_pc_color_to_normal(b, scatter_h):\n",
    "    \"\"\"    \n",
    "        this function is linked to the buttons and updates the the point cloud color\n",
    "    \"\"\"\n",
    "    scatter_h.color = color_n\n",
    "    \n",
    "def update_pc_color_to_curv(b, scatter_h):\n",
    "    \"\"\"    \n",
    "        this function is linked to the buttons and updates the the point cloud color\n",
    "    \"\"\"\n",
    "    scatter_h.color = color_curv\n",
    "\n",
    "#plot\n",
    "fig_h = ipv.figure()\n",
    "scatter_h = ipv.pylab.scatter(point_cloud_dataset.points[:, 0], \n",
    "                              point_cloud_dataset.points[:, 1], \n",
    "                              point_cloud_dataset.points[:, 2], size=0.5, marker=\"sphere\", color='red')\n",
    "ipv.pylab.xyzlim(-1, 1)\n",
    "ipv.style.use('minimal')\n",
    "# ipv.show()\n",
    "\n",
    "btn_pc.on_click(functools.partial(update_pc_color_to_solid, scatter_h=scatter_h))\n",
    "btn_n.on_click(functools.partial(update_pc_color_to_normal, scatter_h=scatter_h))\n",
    "btn_c.on_click(functools.partial(update_pc_color_to_curv, scatter_h=scatter_h))\n",
    "out_widget = widgets.VBox((widgets.HBox((btn_pc, btn_n, btn_c)), fig_h))\n",
    "display(out_widget)\n",
    "\n",
    "# ipv.pylab.savefig(\"figure.png\", fig = fig_h, output_widget= out_widget)\n",
    "\n",
    "# create the colorbar\n",
    "# cbar = ipv.pylab.colorbar(color_n)\n",
    "# set the color of the lines\n",
    "# cbar.solids.set_edgecolor(\"face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points, extracted from numpy array\n",
    "# p = ax.scatter3D(x, y, z, c=z, cmap='winter', s = 5, alpha=0.6) # other cmap options: winter, Greens\n",
    "ax.scatter3D(point_cloud_dataset.points[:, 0], \n",
    "             point_cloud_dataset.points[:, 1], \n",
    "             point_cloud_dataset.points[:, 2], c=color_n, s = 0.5, alpha=0.4) #cmap='winter'\n",
    "\n",
    "# points_new = points_np_array[((points_np_array[:,2] > (z_mean-0.05))),:]\n",
    "# x = points_new[:,0]\n",
    "# y = points_new[:,1]\n",
    "# z = points_new[:,2]\n",
    "# ax.scatter3D(x, y, z, c = 'r', s = 1, alpha=0.2)\n",
    "# cbar = plt.colorbar(p)\n",
    "# cbar.ax.set_ticks(np.arange(0, 2, 0.5))\n",
    "\n",
    "# cbar.set_label(\"Z Height (mm)\", fontsize=16, labelpad=15)\n",
    "# ax.set_xlabel('X (mm)', fontsize=25, labelpad=25)\n",
    "# ax.set_ylabel('Y (mm)', fontsize=25,labelpad=25)\n",
    "# ax.set_zlabel('Z (mm)', fontsize=25, labelpad=20);\n",
    "# ax.set_zticks([0, 0.5, 1, 1.5, 2])\n",
    "# ax.set_xticks([-20, -10, 0, 10, 20])\n",
    "# ax.set_yticks([-30, -20, -10, 0, 10, 20, 30])\n",
    "\n",
    "# for t in ax.zaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "# for t in ax.xaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "# for t in ax.yaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "\n",
    "# plt.title('3D Point cloud and reference plane')\n",
    "ax.view_init(elev=20,azim=-20) #rotate the graph\n",
    "save_fig_plt(\"S_seg_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points, extracted from numpy array\n",
    "# p = ax.scatter3D(x, y, z, c=z, cmap='winter', s = 5, alpha=0.6) # other cmap options: winter, Greens\n",
    "ax.scatter3D(point_cloud_dataset.points[:, 0], \n",
    "             point_cloud_dataset.points[:, 1], \n",
    "             point_cloud_dataset.points[:, 2], c=color_curv, s = 0.5, alpha=0.4) #cmap='winter'\n",
    "\n",
    "# points_new = points_np_array[((points_np_array[:,2] > (z_mean-0.05))),:]\n",
    "# x = points_new[:,0]\n",
    "# y = points_new[:,1]\n",
    "# z = points_new[:,2]\n",
    "# ax.scatter3D(x, y, z, c = 'r', s = 1, alpha=0.2)\n",
    "# cbar = plt.colorbar(p)\n",
    "# cbar.ax.set_ticks(np.arange(0, 2, 0.5))\n",
    "\n",
    "# cbar.set_label(\"Z Height (mm)\", fontsize=16, labelpad=15)\n",
    "# ax.set_xlabel('X (mm)', fontsize=25, labelpad=25)\n",
    "# ax.set_ylabel('Y (mm)', fontsize=25,labelpad=25)\n",
    "# ax.set_zlabel('Z (mm)', fontsize=25, labelpad=20);\n",
    "# ax.set_zticks([0, 0.5, 1, 1.5, 2])\n",
    "# ax.set_xticks([-20, -10, 0, 10, 20])\n",
    "# ax.set_yticks([-30, -20, -10, 0, 10, 20, 30])\n",
    "\n",
    "# for t in ax.zaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "# for t in ax.xaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "# for t in ax.yaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "\n",
    "# plt.title('3D Point cloud and reference plane')\n",
    "ax.view_init(elev=20,azim=-20) #rotate the graph\n",
    "save_fig_plt(\"S_seg_curvature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fit an n-jet using DeepFit (and compute normal vector and principal curvatures)\n",
    "We first load the pretrained model and its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = '../DeepFit/trained_models/DeepFit'\n",
    "params = torch.load(os.path.join(trained_model_path, 'DeepFit_params.pth'))\n",
    "k_neighbors = params.points_per_patch #note you can use a different number, this is what the network trained on\n",
    "jet_order = params.jet_order\n",
    "print('Using {} order jet for surface fitting'.format(jet_order))\n",
    "model = DeepFit.DeepFit(k=1, num_points=k_neighbors, use_point_stn=params.use_point_stn,\n",
    "                        use_feat_stn=params.use_feat_stn, point_tuple=params.point_tuple, sym_op=params.sym_op,\n",
    "                        arch=params.arch, n_gaussians=params.n_gaussians, jet_order=jet_order,\n",
    "                        weight_mode=params.weight_mode, use_consistency=False)\n",
    "checkpoint = torch.load(os.path.join(trained_model_path, 'DeepFit.pth'))\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we use DeepFit to fit a surface at each point in the point cloud. (Use the pre-trained model)\n",
    "This may take a while, depending on the number of points. \n",
    "\n",
    "we also save the results into a txt file (.normal) and (.curv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(point_cloud_dataset, batch_size=128, num_workers=8)\n",
    "\n",
    "num_batch = len(dataloader)\n",
    "\n",
    "for batchind, data in enumerate(dataloader, 0):\n",
    "    points = data[0]\n",
    "    data_trans = data[1]\n",
    "    scale_radius = data[-1].squeeze()\n",
    "    points = points.to(device)\n",
    "    data_trans = data_trans.to(device)\n",
    "    scale_radius = scale_radius.to(device)\n",
    "    n_est, beta, weights, trans, trans2, neighbors_n_est = model.forward(points)\n",
    "    if params.use_point_stn:\n",
    "        n_est = torch.bmm(n_est.unsqueeze(1), trans.transpose(2, 1)).squeeze(dim=1) # cancel out poitnnet stn\n",
    "    # transform predictions with inverse pca rotation (back to world space)\n",
    "    n_est = torch.bmm(n_est.unsqueeze(1), data_trans.transpose(2, 1)).squeeze(dim=1)  # cancel out pca\n",
    "    # print('batchind and batch number[%d/%d]' % (batchind, num_batch-1))\n",
    "\n",
    "    n_est = n_est.detach().cpu()\n",
    "    normals = n_est if batchind == 0 else torch.cat([normals, n_est], 0)\n",
    "    # ---------------------------Save estimated normals to file --------------------------------\n",
    "    # eps=1e-6\n",
    "    # normals[np.logical_and(normals < eps, normals > -eps)] = 0.0\n",
    "    np.savetxt(os.path.join(file_output_dir + '/S_seg.normals'), normals)\n",
    "    \n",
    "    \n",
    "    curv_est, principal_dirs = tu.compute_principal_curvatures(beta)\n",
    "    curv_est = curv_est / scale_radius.unsqueeze(-1).repeat(1, curv_est.shape[1])\n",
    "    curv_est = curv_est.detach().cpu()\n",
    "    curvatures = curv_est if batchind == 0 else torch.cat([curvatures, curv_est], 0)\n",
    "    \n",
    "     # ---------------------------Save estimated normals to file --------------------------------\n",
    "    np.savetxt(os.path.join(file_output_dir + '/S_seg.curv'), curvatures)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normals are unoriented so we will now flip them outwards (assuming that the origin is an internal point and the shape is simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sign = torch.sign(torch.sum(normals*torch.tensor(point_cloud_dataset.points), dim=1)).unsqueeze(-1)\n",
    "normals = n_sign * normals\n",
    "normals = normals.detach().cpu()\n",
    "curvatures = n_sign.repeat([1, 2]) * curvatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the point cloud (DeepFit Network results)\n",
    "We plot the point cloud and allow for 3 color overlays:\n",
    "* Solid - (all points have the same color)\n",
    "* Normals - We map the normal vectors to the RGB cube and use these values for coloring the point cloud.\n",
    "* Curvatures - We map the principal curvatures to the XXX colormap to RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_n = tu.normal2rgb(normals.numpy()) #convert normal vectors to RGB\n",
    "curvature_range_min = [-torch.mean(torch.abs(curvatures[:, 1])) - torch.std(torch.abs(curvatures[:, 1])), \n",
    "                                torch.mean(torch.abs(curvatures[:, 1])) + torch.std(torch.abs(curvatures[:, 1]))]\n",
    "curvature_range_max = [-torch.mean(torch.abs(curvatures[:, 0])) - torch.std(torch.abs(curvatures[:, 0])), \n",
    "                                torch.mean(np.abs(curvatures[:, 0])) + torch.std(torch.abs(curvatures[:, 0]))]                                   \n",
    "color_curv = tu.curvatures2rgb(curvatures.clone(),  k1_range=curvature_range_max, k2_range=curvature_range_min)\n",
    "\n",
    "# make some buttons to toggle between the colors \n",
    "btn_pc = widgets.Button(description='Solid')\n",
    "btn_pc.style.button_color='lightgray'\n",
    "btn_n = widgets.Button(description='normals')\n",
    "btn_n.style.button_color='lightgray'\n",
    "btn_c = widgets.Button(description='Curvatures')\n",
    "btn_c.style.button_color='lightgray'\n",
    "\n",
    "def update_pc_color_to_solid(b, scatter_h):\n",
    "    \"\"\"    \n",
    "        this function is linked to the buttons and updates the the point cloud color\n",
    "    \"\"\"\n",
    "    scatter_h.color = 'red'\n",
    "\n",
    "    \n",
    "def update_pc_color_to_normal(b, scatter_h):\n",
    "    \"\"\"    \n",
    "        this function is linked to the buttons and updates the the point cloud color\n",
    "    \"\"\"\n",
    "    scatter_h.color = color_n\n",
    "\n",
    "def update_pc_color_to_curv(b, scatter_h):\n",
    "    \"\"\"    \n",
    "        this function is linked to the buttons and updates the the point cloud color\n",
    "    \"\"\"\n",
    "    scatter_h.color = color_curv\n",
    "\n",
    "#plot\n",
    "fig_h = ipv.figure()\n",
    "scatter_h = ipv.pylab.scatter(point_cloud_dataset.points[:, 0], \n",
    "                              point_cloud_dataset.points[:, 1], \n",
    "                              point_cloud_dataset.points[:, 2], size=0.5, marker=\"sphere\", color='red')\n",
    "ipv.pylab.xyzlim(-1, 1)\n",
    "ipv.style.use('minimal')\n",
    "# ipv.show()\n",
    "\n",
    "btn_pc.on_click(functools.partial(update_pc_color_to_solid, scatter_h=scatter_h))\n",
    "btn_n.on_click(functools.partial(update_pc_color_to_normal, scatter_h=scatter_h))\n",
    "btn_c.on_click(functools.partial(update_pc_color_to_curv, scatter_h=scatter_h))\n",
    "display(widgets.VBox((widgets.HBox((btn_pc, btn_n, btn_c)), fig_h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points, extracted from numpy array\n",
    "# p = ax.scatter3D(x, y, z, c=z, cmap='winter', s = 5, alpha=0.6) # other cmap options: winter, Greens\n",
    "ax.scatter3D(point_cloud_dataset.points[:, 0], \n",
    "             point_cloud_dataset.points[:, 1], \n",
    "             point_cloud_dataset.points[:, 2], c=color_n, s = 0.5, alpha=0.4) #cmap='winter'\n",
    "\n",
    "# points_new = points_np_array[((points_np_array[:,2] > (z_mean-0.05))),:]\n",
    "# x = points_new[:,0]\n",
    "# y = points_new[:,1]\n",
    "# z = points_new[:,2]\n",
    "# ax.scatter3D(x, y, z, c = 'r', s = 1, alpha=0.2)\n",
    "# cbar = plt.colorbar(p)\n",
    "# cbar.ax.set_ticks(np.arange(0, 2, 0.5))\n",
    "\n",
    "# cbar.set_label(\"Z Height (mm)\", fontsize=16, labelpad=15)\n",
    "# ax.set_xlabel('X (mm)', fontsize=25, labelpad=25)\n",
    "# ax.set_ylabel('Y (mm)', fontsize=25,labelpad=25)\n",
    "# ax.set_zlabel('Z (mm)', fontsize=25, labelpad=20);\n",
    "# ax.set_zticks([0, 0.5, 1, 1.5, 2])\n",
    "# ax.set_xticks([-20, -10, 0, 10, 20])\n",
    "# ax.set_yticks([-30, -20, -10, 0, 10, 20, 30])\n",
    "\n",
    "# for t in ax.zaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "# for t in ax.xaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "# for t in ax.yaxis.get_major_ticks(): t.label.set_fontsize(20)\n",
    "\n",
    "# plt.title('3D Point cloud and reference plane')\n",
    "ax.view_init(elev=20,azim=-20) #rotate the graph\n",
    "save_fig_plt(\"S_seg_DeepFit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
